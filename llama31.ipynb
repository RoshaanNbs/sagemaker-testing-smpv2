{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import boto3\n",
    "from datasets import Dataset, DatasetDict\n",
    "import s3fs \n",
    "from sagemaker.pytorch import PyTorch\n",
    "import sagemaker\n",
    "import os\n",
    "import transformers\n",
    "import logging\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "import functools\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "\n",
    "# Set Hugging Face token\n",
    "HfFolder.save_token('***********************')\n",
    "\n",
    "# Configuration\n",
    "PRETRAINED_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# PRETRAINED_MODEL = \"EleutherAI/gpt-neox-20b\"\n",
    "model_type = \"llama_v3\"  # [gpt_neox, llama_v2]\n",
    "# model_type = \"gpt_neox\"\n",
    "max_context_width = 8192  # For Llama v3 model\n",
    "# max_context_width = 2048 \n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": \"/home/ec2-user/SageMaker/tmp\",\n",
    "}\n",
    "\n",
    "# Set the bucket name and S3 output prefixes\n",
    "bucket_name = \"b-sagemaker\"\n",
    "train_s3_output_prefix = \"fsdp-without-LlamaFactory/hf-llama31-default/datasets/train/\"\n",
    "test_s3_output_prefix = \"fsdp-without-LlamaFactory/hf-llama31-default/datasets/test/\"\n",
    "\n",
    "print(train_s3_output_prefix)\n",
    "print(test_s3_output_prefix)\n",
    "\n",
    "# Create data channels for SageMaker\n",
    "data_channels = {}\n",
    "\n",
    "train = sagemaker.inputs.TrainingInput(\n",
    "    f\"s3://{bucket_name}/{train_s3_output_prefix}\", distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "data_channels[\"train\"] = train\n",
    "\n",
    "test = sagemaker.inputs.TrainingInput(\n",
    "    f\"s3://{bucket_name}/{test_s3_output_prefix}\", distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "data_channels[\"test\"] = test\n",
    "\n",
    "# Print the data channels to verify\n",
    "print('..................................................................')\n",
    "print(data_channels)\n",
    "print('..................................................................')\n",
    "\n",
    "fp8 = 0  # Enable FP8 mixed precision. 1=True, 0=False.\n",
    "tensor_parallel_degree = 4 # An integer in [1, world_size] 8\n",
    "hybrid_shard_degree = (\n",
    "    0  # An integer in [0, world_size // tensor_parallel_degree] and its default value is 0. 1\n",
    ")\n",
    "activation_loading_horizon = (\n",
    "    2  # Activation loading horizon, a positive integer and its default value is 2.\n",
    ")\n",
    "save_steps = 50  # Save step interval.\n",
    "max_steps = 100  # Maximum training steps.\n",
    "offload_activations = True  # Activation offloading.\n",
    "\n",
    "# s3://baykar-sagemaker/fsdp-without-LlamaFactory/hf-llama31-default/datasets/train/\n",
    "\n",
    "hyperparameters = {\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"auto_wrap_policy\": \"transformer_auto_wrap_policy\",\n",
    "    \"backward_fetch_policy\": \"backward_pre\",\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.95,\n",
    "    \"bf16\": 1,\n",
    "    \"checkpoint_dir\": \"s3://b-sagemaker/fsdp-without-LlamaFactory/hf-llama31-default/checkpoints/\",\n",
    "    \"checkpoint_freq\": save_steps,\n",
    "    \"num_kept_checkpoints\": 2,\n",
    "    \"clean_cache\": 0,\n",
    "    \"delayed_param\": 1,\n",
    "    \"enable_memory_profiling\": 0,\n",
    "    \"epochs\": 5,\n",
    "    \"fast_validation\": 0,\n",
    "    \"forward_prefetch\": 1,\n",
    "    \"fp8\": fp8,\n",
    "    \"limit_all_gathers\": 1,\n",
    "    \"logging_freq\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"lr_decay_iters\": 47683,\n",
    "    \"lr_decay_style\": \"cosine\",\n",
    "    \"max_steps\": max_steps,\n",
    "    \"min_lr\": 1e-05,\n",
    "    \"model_type\": model_type,\n",
    "    \"plateau\": 0.0,\n",
    "    \"seed\": 12345,\n",
    "    \"sharding_strategy\": \"hybrid_shard\",\n",
    "    \"train_batch_size\": 16,\n",
    "    \"use_smp_flash_attn\": 1,\n",
    "    \"use_smp_implementation\": 1,\n",
    "    \"val_batch_size\": 4,\n",
    "    \"validation_freq\": save_steps,\n",
    "    \"vocab_size\": 128256,\n",
    "    \"warmup\": 0.0032,\n",
    "    \"weight_decay\": 0.2,\n",
    "    \"zipped_data\": 0,\n",
    "    \"dataset_type\": \"hf\",  \n",
    "    \"distributed_backend\": \"nccl\",\n",
    "    \"model_dir\": \"s3://b-sagemaker/fsdp-without-LlamaFactory/hf-llama31-default/saved_model/\", \n",
    "    \"save_final_model\": 1,\n",
    "    \n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"Training Loss\", \"Regex\": \".*Training Loss: ([0-9\\\\.]+).*\"},\n",
    "    {\"Name\": \"Validation Loss\", \"Regex\": \".*Validation Loss: ([0-9\\\\.]+).*\"},\n",
    "    {\"Name\": \"Learning Rate\", \"Regex\": \".*Learning Rate: ([0-9\\\\.]+).*\"},\n",
    "    {\"Name\": \"Gradient Norm\", \"Regex\": \".*Gradient Norm: ([0-9\\\\.]+).*\"},\n",
    "    {\"Name\": \"Perplexity\", \"Regex\": \".*Perplexity: ([0-9\\\\.]+).*\"},\n",
    "    {\"Name\": \"Accuracy\", \"Regex\": \".*Accuracy: ([0-9\\\\.]+).*\"},\n",
    "    {\"Name\": \"GPU Utilization\", \"Regex\": \".*GPU Utilization: ([0-9\\\\.]+)%.*\"},\n",
    "    {\"Name\": \"Memory Usage\", \"Regex\": \".*Memory Usage: ([0-9\\\\.]+)GB.*\"}\n",
    "]\n",
    "hyperparameters[\"hf_pretrained_model_name_or_dir\"] = PRETRAINED_MODEL\n",
    "# hyperparameters[\"rope_theta\"] = 8.0\n",
    "\n",
    "print(\"*********************1111111111111111111*****************************\")\n",
    "print(PRETRAINED_MODEL)\n",
    "print(\"********************1111111111111111111111****************************************\")\n",
    "\n",
    "\n",
    "# Select your model size.\n",
    "model_config = \"7b\"  # [7b, 65b]\n",
    "\n",
    "if model_type == \"gpt_neox\":\n",
    "    if model_config == \"7b\":\n",
    "        model_params = {\n",
    "            \"max_context_width\": 1024,\n",
    "            \"hidden_width\": 4096,\n",
    "            \"num_layers\": 32,\n",
    "            \"num_heads\": 32,\n",
    "        }\n",
    "    elif model_config == \"20b\":\n",
    "        model_params = {\n",
    "            \"max_context_width\": 2048,\n",
    "            \"hidden_width\": 6144,\n",
    "            \"num_layers\": 44,\n",
    "            \"num_heads\": 64,\n",
    "        }\n",
    "    elif model_config == \"65b\":\n",
    "        model_params = {\n",
    "            \"max_context_width\": 1024,\n",
    "            \"hidden_width\": 8192,\n",
    "            \"num_layers\": 80,\n",
    "            \"num_heads\": 64,\n",
    "        }\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown model config\")\n",
    "elif model_type == \"llama_v3\":\n",
    "    if model_config == \"7b\":\n",
    "        model_params = {\n",
    "            \"max_context_width\": 8192,\n",
    "            \"hidden_width\": 4096,\n",
    "            \"num_layers\": 32,\n",
    "            \"num_heads\": 32,\n",
    "            \"llama_intermediate_size\": 14336,\n",
    "        }\n",
    "        \n",
    "    # elif model_config == \"8b\":\n",
    "    #     model_params = {\n",
    "    #         \"max_context_width\": 8192,\n",
    "    #         \"hidden_width\": 4096,\n",
    "    #         \"num_layers\": 40, \n",
    "    #         \"num_heads\": 32,   \n",
    "    #         \"llama_intermediate_size\": 14336,\n",
    "    #     }        \n",
    "        \n",
    "    elif model_config == \"65b\":\n",
    "        model_params = {\n",
    "            \"max_context_width\": 4096,\n",
    "            \"hidden_width\": 8192,\n",
    "            \"num_layers\": 80,\n",
    "            \"num_heads\": 64,\n",
    "            \"llama_intermediate_size\": 22016,\n",
    "        }\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown model config\")\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v\n",
    "\n",
    "print(hyperparameters)\n",
    "\n",
    "\n",
    "instance_type = \"ml.p4de.24xlarge\"\n",
    "\n",
    "# You need >= 1 p4d for 7b model.\n",
    "# You need >= 8 p4d for 65b model.\n",
    "instance_count = 1\n",
    "\n",
    "# set to the number of GPUs on that instance\n",
    "processes_per_host = 8\n",
    "\n",
    "# Assuming you will run this code on your local machine\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "# default_bucket = sagemaker_session.default_bucket()\n",
    "# print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "# base_job_name = f'smp-{model_config}-{machine_str}-hs{hybrid_shard_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]}'\n",
    "base_job_name = \"baykar-fsdp-without-llamafactory-model-training\"\n",
    "\n",
    "print(base_job_name)\n",
    "\n",
    "# checkpoint_s3_uri = \"s3://tubitak-exp-v1/Exp1-TubitakV0-qa1m-tr-instruction-formatted/checkpoints/\"\n",
    "s3_output_bucket = \"s3://baykar-sagemaker/fsdp-without-LlamaFactory/hf-llama31-default/saved_model/\"\n",
    "\n",
    "# # Pytorch Estimator\n",
    "\n",
    "kwargs = {}\n",
    "tags={'Key': 'CostId', 'Value': 'Baykar_NBS'}\n",
    "\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {                        # Required\n",
    "        \"pipeline_parallel_degree\": 2,     # Required\n",
    "        \"microbatches\": 4,\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,                      # Required\n",
    "    \"processes_per_host\" : 8,              # Required\n",
    "    # \"custom_mpi_options\" : \"--mca btl_vader_single_copy_mechanism none\"\n",
    "}\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    # checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    checkpoint_local_path=\"/opt/ml/checkpoints\",\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,  \n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri='658645717510.dkr.ecr.us-east-1.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121',\n",
    "    tags=tags,\n",
    "#     distribution={\n",
    "        \n",
    "#         \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "#         \"mpi\": mpi_options\n",
    "#     },\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\"enabled\": True}, \n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"tensor_parallel_degree\": tensor_parallel_degree,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
    "                    \"sm_activation_offloading\": offload_activations,\n",
    "                    \"activation_loading_horizon\": activation_loading_horizon\n",
    "                    # \"smp_rope_enabled\": True\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "  \n",
    "    \n",
    "    \n",
    "    # py_version='py310',\n",
    "    # framework_version=\"2.4.0\",\n",
    "    # image_uri=$IMAGE,  # Either provide `framework_version` or `image_uri`\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    metric_definitions=metric_definitions,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "# # Make sure data channels are ready\n",
    "\n",
    "print(data_channels)\n",
    "\n",
    "import sys\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "# Specify the file to save the logs\n",
    "log_file_path = \"exp-v2.txt\"\n",
    "\n",
    "\n",
    "smp_estimator.fit(inputs=data_channels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
